{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ive9tPJ4PEtq"
   },
   "source": [
    "**Classify insults using glove and attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "GuOp-KBo6zj4",
    "outputId": "78ae787b-2a9d-449d-e5c2-03e4bdb6a540",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tV2BnA6Z7Ubj"
   },
   "outputs": [],
   "source": [
    "dataset_path = 'gdrive/My Drive/Colab/insult/'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv(dataset_path + \"train_text.csv\", encoding = 'latin-1')\n",
    "test_df = pd.read_csv(dataset_path + \"test_text.csv\", encoding = 'latin-1')\n",
    "train_id, X_train, y_train = train_df['id'], train_df['Comment'], train_df['Target']\n",
    "test_id, X_test = test_df['id'], test_df['Comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hO4fjy1TR6b7"
   },
   "source": [
    "**Standard preprocessing applied** (some have been commented based on results and observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "p_5OQZOP7jjp",
    "outputId": "b9c633e8-2b9f-4d59-932b-681b4c63bfd7"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "X_train_mod, y_train_mod = [], []\n",
    "X_test_mod= []\n",
    "words = []\n",
    "table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "for ind, review in enumerate(X_train):\n",
    "  try:\n",
    "    #review = cleanData(review)\n",
    "    word_review = word_tokenize(review)\n",
    "    \n",
    "    word_review = [w.translate(table) for w in word_review]\n",
    "    #word_review = [w for w in word_review if w.isalpha()]\n",
    "    #word_review = [w for w in word_review if w not in stop_words]\n",
    "    word_review = [w.lower() for w in word_review]\n",
    "    #word_review = [lmtzr.lemmatize(w) for w in word_review]\n",
    "    #word_review = list(set(word_review))\n",
    "    text_joined = ' '.join(word_review)\n",
    "    X_train_mod.append(text_joined)\n",
    "    y_train_mod.append(y_train[ind])\n",
    "    \n",
    "    words += word_review\n",
    "  except:\n",
    "    \n",
    "    continue\n",
    "for ind, review in enumerate(X_test):\n",
    "  try:\n",
    "    #review = cleanData(review)\n",
    "    word_review = word_tokenize(review)\n",
    "    \n",
    "    word_review = [w.translate(table) for w in word_review]\n",
    "    #word_review = [w for w in word_review if w.isalpha()]\n",
    "    #word_review = [w for w in word_review if w not in stop_words]\n",
    "    word_review = [w.lower() for w in word_review]\n",
    "    #word_review = [lmtzr.lemmatize(w) for w in word_review]\n",
    "    #word_review = list(set(word_review))\n",
    "    text_joined = ' '.join(word_review)\n",
    "    X_test_mod.append(text_joined)\n",
    "    #y_test_mod.append(y_test[ind])\n",
    "    words += word_review\n",
    "  except:\n",
    "    continue\n",
    "    \n",
    "X_total = X_train_mod + X_test_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WvFpcWkOcr-F",
    "outputId": "0c9ec454-75f4-4092-d81d-2642a90c38ca"
   },
   "outputs": [],
   "source": [
    "print(len(X_test_mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UTw6fBkWSILM"
   },
   "source": [
    "**Using Glove 300d**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_0B8qGlf732F"
   },
   "outputs": [],
   "source": [
    "embedding = {}\n",
    "f = open(dataset_path + \"glove.840B.300d.txt\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "      coefs = np.asarray(values[1:], dtype='float32')\n",
    "      embedding[word] = coefs\n",
    "    except:\n",
    "      continue\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V6wrWwK6Ugxj"
   },
   "source": [
    "**Keras Processing and glove vectorization**\n",
    "The most notable point here is we have padded the sequences in two ways: Pre and post. We are going to use them in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "gAqGYC3y78J4",
    "outputId": "25768d7f-5f6d-4e25-81e6-f9080e5bee87"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#max_len = max(len(l.split()) for l in X_total)\n",
    "max_len = 200\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_total)\n",
    "sequences = tokenizer.texts_to_sequences(X_train_mod)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "\n",
    "review_pad = pad_sequences(sequences, maxlen = max_len)\n",
    "review_pad_post = pad_sequences(sequences, maxlen=max_len,padding='post', truncating='post')\n",
    "print(review_pad.shape)\n",
    "print(review_pad_post.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1gAVBdx8AZp"
   },
   "outputs": [],
   "source": [
    "y_train_mod = np.asarray(y_train_mod).reshape(len(y_train_mod), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "a_6WQ2az8DEe",
    "outputId": "a044e65e-82f7-42e7-fa0a-d242fb29eb61"
   },
   "outputs": [],
   "source": [
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, 300))\n",
    "for word, i in word_index.items():\n",
    "  if i > num_words:\n",
    "    continue\n",
    "  embedding_vector = embedding.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wUrhSRXWXHe"
   },
   "source": [
    "**The Central Model**\n",
    "\n",
    "Based on the pre and post padding, we have two branches now. For each branch, we send the input layer through an LSTM layer with dropout before sending it to the attention block. Post this procedure for each block, we concatenate the blocks, flatten them, add a dense layer, and finally the output layer is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CUZnqWBb8Fjv"
   },
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.initializers import Constant\n",
    "\n",
    "TIME_STEPS = max_len\n",
    "INPUT_DIM = max_len\n",
    "\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])//2\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(TIME_STEPS*2, activation='relu')(a)\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    return output_attention_mul\n",
    "\n",
    "\n",
    "def model_attention_applied_after_lstm():\n",
    "    input_pre = Input(shape= (INPUT_DIM,))\n",
    "    input_post = Input(shape= (INPUT_DIM,))\n",
    "    lstm_units = 32\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                300,\n",
    "                                embeddings_initializer = Constant(embedding_matrix),\n",
    "                                input_length = max_len,\n",
    "                                trainable=False)\n",
    "    \n",
    "    inputs_1 = embedding_layer(input_pre)\n",
    "    inputs_1 = SpatialDropout1D(0.25)(inputs_1)\n",
    "    lstm_out_1 = Bidirectional(CuDNNLSTM(lstm_units,return_sequences=True))(inputs_1)\n",
    "    lstm_out_1 = Dropout(0.5)(lstm_out_1)\n",
    "    x1 = attention_3d_block(lstm_out_1)\n",
    "\n",
    "    inputs_2 = embedding_layer(input_post)\n",
    "    inputs_2 = SpatialDropout1D(0.25)(inputs_2)\n",
    "    lstm_out_2 = Bidirectional(CuDNNLSTM(lstm_units,return_sequences=True))(inputs_2)\n",
    "    lstm_out_2 = Dropout(0.5)(lstm_out_2)\n",
    "    x2 = attention_3d_block(lstm_out_2)\n",
    "\n",
    "    x = concatenate([x1, x2])\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    preds = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=[input_pre, input_post], outputs=preds)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZhDuijT8MP9"
   },
   "outputs": [],
   "source": [
    "m = model_attention_applied_after_lstm()\n",
    "\n",
    "#m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "V5Wq8WDj8Pxh",
    "outputId": "94aead5d-ceca-4a49-d309-1e64772286d6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(review_pad, \n",
    "                                                    y_train_mod, test_size=0.1,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y_train_mod)\n",
    "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(review_pad_post, \n",
    "                                                    y_train_mod, test_size=0.1,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y_train_mod)\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LTV4zffWIZPO"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint_path = dataset_path + \"model-{epoch:02d}.h5\"\n",
    "\n",
    "cp_callback = ModelCheckpoint(checkpoint_path, verbose=1,\n",
    "                              save_best_only=True,\n",
    "                              monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gD3qWzHl8VQ8"
   },
   "outputs": [],
   "source": [
    "m.fit([X_train,X_train_p], y_train, batch_size = 128, epochs = 10,\n",
    "      validation_data = ([X_test,X_test_p], y_test),\n",
    "          callbacks = [], verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "U_wXExNw9lBw",
    "outputId": "8971b20d-e3fe-4d19-ef3b-7e2b8b2f5ba0"
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_test_mod)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "\n",
    "test_pad = pad_sequences(sequences, maxlen = max_len)\n",
    "test_pad_post = pad_sequences(sequences, maxlen=max_len,padding='post', truncating='post')\n",
    "print(test_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W0yGsv6rRIj_"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"gdrive/My Drive/Colab/insult/model-07.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "B6YITOc-RRvF",
    "outputId": "a7ce7aae-a7fe-496a-83e8-994450ec4f66"
   },
   "outputs": [],
   "source": [
    "output = model.predict([test_pad,test_pad_post], verbose=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q47WfcM1RXXW"
   },
   "outputs": [],
   "source": [
    "output = [l[0] for l in output]\n",
    "output = list(output)\n",
    "out_df = pd.DataFrame(list(zip(test_id, output)), columns =['id', 'Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1RbXeUuaRgg4"
   },
   "outputs": [],
   "source": [
    "print(out_df)\n",
    "out_df.to_csv(\"output_attn_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swgXa3oDStd-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "first.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
